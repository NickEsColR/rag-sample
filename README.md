# DocSageğŸ§™â€â™‚ï¸ - RAG Sample Application

DocSage es una aplicaciÃ³n de chat inteligente que utiliza Retrieval-Augmented Generation (RAG) para responder preguntas basadas en documentos y enlaces web que subas. La aplicaciÃ³n permite crear mÃºltiples chats, cargar documentos de diferentes formatos, agregar enlaces web como fuentes de informaciÃ³n, y mantener conversaciones contextuales con un modelo de IA.

## ğŸš€ CaracterÃ­sticas

- **Chat MÃºltiple**: Crea y gestiona mÃºltiples conversaciones independientes
- **Carga de Documentos**: Soporte para archivos PDF, DOCX, TXT, CSV, HTML y Markdown
- **IntegraciÃ³n Web**: Agrega enlaces web como fuentes de informaciÃ³n
- **RAG (Retrieval-Augmented Generation)**: Respuestas contextuales basadas en tus documentos
- **Interfaz Intuitiva**: AplicaciÃ³n web construida con Streamlit
- **Base de Datos**: Almacenamiento persistente con SQLite
- **Embeddings**: Utiliza modelos de Hugging Face para generar embeddings
- **BÃºsqueda SemÃ¡ntica**: BÃºsqueda vectorial con Chroma

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Frontend**: Streamlit
- **Backend**: Python con LangChain
- **Base de Datos**: SQLite
- **Vector Store**: Chroma
- **Embeddings**: HuggingFace (sentence-transformers/all-mpnet-base-v2)
- **LLM**: Azure AI via GitHub Models (mistral-ai/mistral-medium-2505)
- **GestiÃ³n de Dependencias**: UV

## âš™ï¸ ConfiguraciÃ³n

### 1. Crear archivo de configuraciÃ³n

Crea un archivo `.env` en la raÃ­z del proyecto con las siguientes variables:

```env
# Token de GitHub para acceder a GitHub Models
GITHUB_TOKEN=your_github_token_here
```

### 2. Obtener GitHub Token

Para usar los modelos del marketplace de GitHub Models, necesitas un token de GitHub con permisos especÃ­ficos:

1. Ve a [GitHub Settings > Developer Settings > Personal Access Tokens](https://github.com/settings/tokens)
2. Haz clic en "Generate new token" > "Generate new token (classic)"
3. Asigna un nombre descriptivo al token (ej: "DocSage RAG App - GitHub Models")
4. Selecciona los siguientes scopes necesarios para acceder al marketplace de modelos:
   - `read:user` (para informaciÃ³n bÃ¡sica del usuario)
   - `user:email` (para acceder al email del usuario)
   - `repo` (requerido para acceder a GitHub Models marketplace)
5. Haz clic en "Generate token"
6. **Â¡IMPORTANTE!** Copia el token inmediatamente ya que no podrÃ¡s verlo de nuevo
7. Pega el token en tu archivo `.env` como valor de `GITHUB_TOKEN`

**Nota:** El scope `repo` es necesario para autenticarte con GitHub Models y acceder a los modelos disponibles en el marketplace como Mistral AI Medium 2505.

### 3. InstalaciÃ³n de dependencias

AsegÃºrate de tener [UV](https://docs.astral.sh/uv/) instalado. Si no lo tienes:

```bash
# En macOS y Linux:
curl -LsSf https://astral.sh/uv/install.sh | sh

# En Windows (PowerShell):
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

Luego instala las dependencias:

```bash
uv sync
```

## ğŸš€ InstalaciÃ³n y EjecuciÃ³n

### 1. Crear la base de datos

Ejecuta el script para crear la base de datos SQLite y las tablas necesarias:

```bash
uv run python create_relational_db.py
```

Este comando crearÃ¡:

- Un archivo `doc_sage.sqlite` con las tablas para chats, mensajes y fuentes
- Tablas para gestionar conversaciones y documentos asociados

### 2. Ejecutar la aplicaciÃ³n

Inicia la aplicaciÃ³n web con Streamlit:

```bash
uv run streamlit run chats.py
```

La aplicaciÃ³n estarÃ¡ disponible en `http://localhost:8501`

## ğŸ“– Uso de la AplicaciÃ³n

### Crear un nuevo chat

1. En la pÃ¡gina principal, introduce un tÃ­tulo para tu chat
2. Haz clic en "Create Chat"

### Agregar documentos

1. En el sidebar del chat, usa el uploader para subir documentos
2. Formatos soportados: PDF, DOCX, TXT, CSV, HTML, MD

### Agregar enlaces web

1. En el sidebar, introduce una URL en el campo "Add a link"
2. Haz clic en "Add Link" para extraer y procesar el contenido

### Conversar

1. Escribe tu pregunta en el campo de chat
2. El sistema buscarÃ¡ informaciÃ³n relevante en tus documentos/enlaces
3. RecibirÃ¡s una respuesta contextualizada basada en tus fuentes

## ğŸ“ Estructura del Proyecto

```text
rag-sample/
â”œâ”€â”€ chats.py              # AplicaciÃ³n principal de Streamlit
â”œâ”€â”€ db.py                 # Operaciones de base de datos
â”œâ”€â”€ vector_functions.py   # Funciones para manejo de vectores y RAG
â”œâ”€â”€ create_relational_db.py # Script para crear la base de datos
â”œâ”€â”€ pyproject.toml        # ConfiguraciÃ³n del proyecto y dependencias
â”œâ”€â”€ .env                  # Variables de entorno (crear manualmente)
â”œâ”€â”€ doc_sage.sqlite       # Base de datos SQLite (se crea automÃ¡ticamente)
â””â”€â”€ persist/              # Directorio para almacenar vectores (se crea automÃ¡ticamente)
```

## ğŸ”§ Dependencias Principales

- `streamlit`: Interface web interactiva
- `langchain`: Framework para aplicaciones LLM
- `langchain-azure-ai`: IntegraciÃ³n con modelos de Azure
- `langchain-chroma`: Vector store para bÃºsqueda semÃ¡ntica
- `langchain-huggingface`: Embeddings de Hugging Face
- `python-environ`: Manejo de variables de entorno
- `beautifulsoup4`: Parser para contenido web

## ğŸ“ Notas Adicionales

- Los documentos se procesan en chunks de 1000 caracteres para mejor rendimiento
- La aplicaciÃ³n utiliza embeddings `sentence-transformers/all-mpnet-base-v2`
- Los chats son independientes con sus propios contextos vectoriales
- La base de datos SQLite almacena metadatos, mientras que Chroma almacena los vectores

## ğŸ™ Reconocimientos

Este proyecto estÃ¡ basado en el excelente tutorial **"Create a Smart RAG App with LangChain and Streamlit"** de [Ngonidzashe Nzenze](https://dev.to/ngonidzashe), publicado en DEV Community. El tutorial original proporciona una guÃ­a completa paso a paso para crear una aplicaciÃ³n RAG inteligente utilizando LangChain y Streamlit.

**Enlace al tutorial original:** <https://dev.to/ngonidzashe/doc-sage-create-a-smart-rag-app-with-langchain-and-streamlit-4lin>

**Autor:** Ngonidzashe Nzenze  
**Publicado:** 6 de noviembre de 2024

### Adaptaciones en esta versiÃ³n

- **Cambio de modelo LLM**: Se migrÃ³ de OpenAI GPT-4o-mini a **Mistral AI Medium 2505** a travÃ©s de GitHub Models
- **Cambio de embeddings**: Se cambiÃ³ de OpenAI Embeddings a **Hugging Face sentence-transformers**
- **Sistema de gestiÃ³n**: Se integrÃ³ **UV** como gestor de dependencias
- **Mejoras en documentaciÃ³n**: README expandido con guÃ­as detalladas de configuraciÃ³n

Agradecemos a Ngonidzashe por compartir su conocimiento y crear un tutorial tan claro y educativo que sirve como base para este proyecto.
